{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web Scraping / Data Extraction using Python and Saved in CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://islamqa.info/en/answers/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see the content of entire page use below query\n",
    "# page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below command will give clean output of all the content of pages\n",
    "soup = bs(page.content,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_all function will return array if there is one element or multiple emelemts\n",
    "questionanswer = soup.find_all(attrs= {'class':'content'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'InterruptionofWudhu'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = soup.find(attrs={\"class\":'title is-4 is-size-5-touch'}).text.replace('\\n', '').replace(' ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "QuestionNo = int(soup.find(attrs={\"class\":'subtitle has-text-weight-bold has-title-case cursor-pointer tooltip'}).text.replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = soup.find(attrs={'class':'subtitle is-6 has-text-weight-bold is-capitalized'}).text.replace('\\n','').replace('Source:','').replace(' ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now to store the data we will use panda and store that in a list\n",
    "data = [[url, QuestionNo, summary, source ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will use a Dataframe of pandas\n",
    "df = pd.DataFrame(data, columns=['url', 'Question #', 'Summary', 'Source'])\n",
    "\n",
    "# this work is done for a single url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetched successfully  1\n",
      "Data fetched successfully  2\n",
      "Data fetched successfully  3\n",
      "Data fetched successfully  4\n",
      "Data fetched successfully  5\n",
      "Data fetched successfully  6\n",
      "Data fetched successfully  7\n",
      "Data fetched successfully  8\n",
      "Data fetched successfully  9\n"
     ]
    }
   ],
   "source": [
    "# for multiple urls\n",
    "for i in range(1, 10):\n",
    "    URL = 'https://islamqa.info/en/answers/' +str(i)\n",
    "    page = requests.get(URL)\n",
    "    if(page.status_code == 200):\n",
    "        print('Data fetched successfully ', i)\n",
    "        soup = bs(page.content,'html.parser')\n",
    "        questionanswer = soup.find_all(attrs= {'class':'content'})\n",
    "        summary = soup.find(attrs={\"class\":'title is-4 is-size-5-touch'}).text.replace('\\n', '').replace(' ','')\n",
    "        QuestionNo = int(soup.find(attrs={\"class\":'subtitle has-text-weight-bold has-title-case cursor-pointer tooltip'}).text.replace('\\n',''))\n",
    "        source = soup.find(attrs={'class':'subtitle is-6 has-text-weight-bold is-capitalized'}).text.replace('\\n','').replace('Source:','').replace(' ','')\n",
    "        data.insert(QuestionNo, [url, QuestionNo, summary, source ])\n",
    "    else:\n",
    "        print('URL Not Found', i)\n",
    "df = pd.DataFrame(data, columns=['url', 'Question #', 'Summary', 'Source'])\n",
    "df.to_csv('pagedata.csv')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5cd780aa707927d7ead15f3bbdb3bbd8465e273aa5c683acfbaf864c62a1c2f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
